---
tags:
  - open-source
  - fine-tuned
  - llama-based
  - conversational
---

# Vicuna

An open-source chatbot fine-tuned from LLaMA using conversation data shared by users. Developed by UC Berkeley, CMU, Stanford, and UC San Diego, known for achieving 90% of ChatGPT's quality.

## Key Features
- **Conversation Optimized**: Fine-tuned on user conversations
- **High Quality**: 90% of ChatGPT's performance claimed
- **Open Source**: Based on LLaMA with additional training
- **Multiple Sizes**: 7B, 13B, and 33B versions
- **Cost Effective**: Training cost only ~$300

## Development
- **Base Model**: LLaMA foundation
- **Training Data**: 70K user conversations from ShareGPT
- **Fine-tuning**: Supervised instruction tuning
- **Evaluation**: GPT-4 based assessment

## Model Versions
- **Vicuna-7B**: Compact version for local use
- **Vicuna-13B**: Balanced performance and efficiency
- **Vicuna-33B**: Highest quality conversations

## Capabilities
- High-quality conversations
- Multi-turn dialogue
- Instruction following
- Question answering
- Creative writing
- Code assistance (basic)

## Performance
- Competitive with ChatGPT on many tasks
- Strong conversational abilities
- Good instruction following
- Natural dialogue flow
- Context awareness

## Training Details
- **Dataset**: ShareGPT conversations
- **Method**: Supervised fine-tuning
- **Cost**: ~$300 for training
- **Evaluation**: GPT-4 judge assessment

## Local Deployment
- Compatible with [[Ollama]]
- Text generation web UI support
- Various quantization options
- FastChat framework integration

## Use Cases
- Personal AI assistants
- Conversational interfaces
- Educational chatbots
- Customer service bots
- Research into dialogue systems
- Local ChatGPT alternative

## Hardware Requirements
- **7B Model**: 8GB+ RAM
- **13B Model**: 16GB+ RAM
- **33B Model**: 20GB+ RAM
- GPU acceleration recommended

## Advantages
- Excellent conversational quality
- Low training cost replication
- Strong community support
- Good performance for size
- Open research and development

## Limitations
- Based on potentially outdated LLaMA
- Limited compared to latest models
- Conversation data quality varies
- Less specialized than domain-specific models

## Community
- Active research community
- Regular model updates
- Open evaluation methodology
- Reproducible training process

Back to [[models]]